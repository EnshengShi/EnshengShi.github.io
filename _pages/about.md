---
permalink: /
title: "About me"
excerpt: "About me"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

I am a joint PhD student, supervised by Dongmei Zhang, Shi Han, Yanlin Wang and Prof. Hongbin Sun, between Microsoft Research Asia and Xi'an Jiaotong University. 

I am mainly focusing on **code intelligence** (intersection of Software Engineering and Artificial Intelligence), which leverages artificial intelligence approaches to analyze and model source code and its related artifacts. Specifically, it utilizes the **machine learning** model to mine knowledge from large scale, free source code data (Big Code) which is available on Github, etc, obtains the better **code representation** (based on code token/ AST/ PDG/ IR etc) and applies the representation to the **downstream tasks** such as code summarization, code search, clone detection, code completion, program repair, etc.

My research areas currently include: **(1) Code Represention Learning; (2) Code Summarization; (3) Code Search; (4)Commit Message Generation**



## Publications

### 2023
<p><b>CoCoAST: Representing Source Code via Hierarchical Splitting and Reconstruction of Abstract Syntax Trees</b> 
<br><small>
<i>EMSE  (<b>CCF-B</b>) <a href="https://link.springer.com/article/10.1007/s10664-023-10378-9">[pdf]</a> <a href="https://github.com/s1530129650/CoCoAST">[code]</a></i> 
<br />
<u>Ensheng Shi</u>, Yanlin Wang, Lun Du, Hongyu Zhang, Shi Han, Dongmei Zhang, Hongbin Sun
<br /><b>TLNR</b>: We propose a novel model CoCoAST that hierarchically splits and reconstructs ASTs to comprehensively capture the syntactic and semantic information of code without the loss of AST structural information. We have applied our source code representation to two common program comprehension tasks, code summarization and code search. </small>
</p>

<p><b>You Augment Me: Exploring ChatGPT-based Data Augmentation for Semantic Code Search</b> <br>
<small>
<i>ICSME2023 (<b>CCF-B</b>) <a href="https://conf.researchr.org/details/icsme-2023/icsme-2023-papers/18/You-Augment-Me-Exploring-ChatGPT-based-Data-Augmentation-for-Semantic-Code-Search">[pdf]</a> <a href="">[code]</a></i>
<br />
Yanlin Wang, Lianghong Guo, <u>Ensheng Shi*</u>, Wenqing Chen, Jiachi Chen, Wanjun Zhong, Menghan Wang, Hui Li, Ziyu Lyu, Hongyu Zhang, Zibin Zheng
<br /><b>TLNR</b>: We propose a novel approach ChatDANCE, which utilizes high-quality and diverse augmented data generated by a large language model and leverages a filtering mechanism to eliminate low-quality augmentations.
<br />
</small>
</p>


<p><b>Towards Efficient Fine-tuning of Pre-trained Code Models: An Experimental Study and Beyond</b> <br>
<small>
<i>ISSTA2023 (<b>CCF-A</b>) <a href="https://arxiv.org/abs/2304.05216">[pdf]</a> <a href="https://github.com/DeepSoftwareAnalytics/Telly">[code]</a></i>
<br />
<u>Ensheng Shi</u>, Yanlin Wang, Hongyu Zhang, Lun Du, Shi Han, Dongmei Zhang, Hongbin Sun
<br /><b>TLNR</b>: we conduct an extensive experimental study to explore what happens to layer-wise pre-trained representations and their encoded code knowledge during fine-tuning. We then propose efficient alternatives to fine-tune the large pre-trained code model based on the above findings.
<br />
<details>
<summary></summary>
<ul>
<li> lexical, syntactic and structural properties of source code are encoded in the lower, intermediate, and higher layers, respectively, while the semantic property spans across the entire model.</li>
<li>The process of fine-tuning preserves most of the code properties. Specifically, the basic code properties captured by lower and intermediate layers are still preserved during fine-tuning. Furthermore, we find that only the representations of the top two layers change most during fine-tuning for various downstream tasks. 
</li>
<li>Based on the above findings, we propose Telly to efficiently fine-tune pre-trained code models via layer freezing. 
</li>
</ul>
</details>
</small>
</p>


<p><b>CoCoSoDa: Effective Contrastive Learning for Code Search</b> <br>
<small>
<i>ICSE2023 (<b>CCF-A</b>)<a href="https://arxiv.org/abs/2204.03293">[pdf]</a> <a href="https://github.com/DeepSoftwareAnalytics/RACE">[code]</a></i>
<br />
<u>Ensheng Shi</u>, Wenchao Gu, Yanlin Wang, Lun Du, Hongyu Zhang, Shi Han, Dongmei Zhang, Hongbin Sun
<br /><b>TLNR</b>: We propose CoCoSoDa to effectively utilize contrastive learning for code search via two key factors in contrastive learning: data augmentation and negative samples.
<br />
<details>
<summary></summary>
<ul>
<li>CoCoSoDa outperforms 14 baselines and especially exceeds CodeBERT, GraphCodeBERT, and UniXcoder by 13.3%, 10.5%, and 5.9% on average MRR scores, respectively.</li>
<li> The ablation studies show the effectiveness of each component of our approach.
</li>
<li>We adapt our techniques to several different pre-trained models such as RoBERTa, CodeBERT, and GraphCodeBERT and observe a significant boost in their performance in code search. </li>
<li>Our model performs robustly under different hyperparameters. Furthermore, we perform qualitative and quantitative analyses to explore reasons behind the good performance of our model.
</li>
</ul>
</details>
</small>
</p>

### 2022
<p><b>RACE: Retrieval-augmented Commit Message Generation</b> 
<br><small>
<i>EMNLP2022 (<b>CCF-B</b>)<a href="https://arxiv.org/abs/2203.02700">[pdf]</a> <a href="https://github.com/DeepSoftwareAnalytics/RACE">[code]</a></i>
<br />
<u>Ensheng Shi</u>, Yanlin Wang, Wei Tao, Lun Du, Hongyu Zhang, Shi Han, Dongmei Zhang, Hongbin Sun
<br /><b>TLNR</b>: We propose a new retrieval-augmented neural commit message generation method, which treats the retrieved similar commit as an exemplar and leverages it to generate an accurate commit message.  </small>
</p>


<p><b>A Large-Scale Empirical Study of Commit Message Generation: Models, Datasets and Evaluation</b> 
<br><small>
<i>EMSE2022 (<b>CCF-B</b>) <a href="https://link.springer.com/article/10.1007/s10664-022-10219-1">[pdf]</a> <a href="https://github.com/DeepSoftwareAnalytics/CommitMsgEmpirical">[code]</a></i>
<br />
 Wei Tao, Yanlin Wang, <u>Ensheng Shi</u>, Lun Du, Shi Han, Hongyu Zhang, Dongmei Zhang, Wenqiang Zhang
<br /><b>TLNR</b>: To achieve a better understanding of how the existing approaches perform in solving this problem, this paper conducts a systematic and in-depth analysis of the state-of-the-art models and datasets.</small>
</p>

<p><b>On the Evaluation of Neural Code Summarization</b> <br>
<small>
<i>ICSE2022  (<b>CCF-A</b>) <a href="https://arxiv.org/abs/2107.07112">[pdf]</a> <a href="https://github.com/DeepSoftwareAnalytics/CodeSumEvaluation">[code]</a><a href="https://zhuanlan.zhihu.com/p/443994857">[blog]</a></i>
<br />
<u>Ensheng Shi</u>, Yanlin Wang, Lun Du, Junjie Chen, Shi Han, Hongyu Zhang, Dongmei Zhang, Hongbin Sun 
<br /><b>TLNR</b>: Some interesting and surprising findings on the evaluated metric, code-preprocessing, and evaluated datasets. Building a shared code summarization toolbox andgiving actionable suggestions on the evaluation of neural code summarization. 
<br />
<details>
<summary></summary>
<ul>
<li> The BLEU metric widely used in existing work of evaluating code summarization models has many variants. Ignoring the differences among these variants could greatly affect the validity of the claimed results.</li>
<li> BLEU_DC (sentence BLEU with smoothing method 4) is most correlated to human perception on the evaluation of neural code summarization model among the 6 widely used BLEU variants.
</li>
<li> Code pre-processing choices can have a large (from -18\% to +25\%) impact on the summarization performance and should not be neglected.</li>
<li> Performing S (identifier splitting) is always significantly better than not performing it. And different code pre-processing has a large impact on performance (-18\% to +25\%)
</li>
<li> 
Some important characteristics of datasets (corpus sizes, data splitting methods, and duplication ratios) have a significant impact on model evaluation.  
</li> 
<li> Based on the experimental results, we give actionable suggestions for evaluating code summarization and choosing the best method in different scenarios. We also build a shared code summarization toolbox to facilitate future research.
</li>
</ul>
</details>
</small>
</p>

### 2021



<p><b>CAST: Enhancing Code Summarization with Hierarchical Splitting and Reconstruction of Abstract Syntax Trees</b> 
<br><small>
<i>EMNLP2021  (<b>CCF-B</b>) <a href="https://aclanthology.org/2021.emnlp-main.332.pdf">[pdf]</a> <a href="https://github.com/DeepSoftwareAnalytics/CAST">[code]</a></i>
<br />
<u>Ensheng Shi</u>, Yanlin Wang, Lun Du, Hongyu Zhang, Shi Han, Dongmei Zhang, Hongbin Sun
<br /><b>TLNR</b>: Our model hierarchically
splits and reconstructs ASTs to obtain the better code representation for code summarization.</small>
</p>

<p><b>Is a Single Model Enough? MuCoS: A Multi-Model Ensemble Learning Approach for Semantic Code Search</b> 
<br><small>
<i>CIKM 2021 (<b>CCF-B</b>) <a href="https://dl.acm.org/doi/abs/10.1145/3459637.3482127">[pdf]</a> <a href="https://github.com/Xzh0u/MuCoS">[code]</a></i>
<br />
 Lun Du, Xiaozhou Shi, Yanlin Wang, <u>Ensheng Shi</u>,  Shi Han, Dongmei Zhang
<br /><b>TLNR</b>: Ensembling three models which focus on the structure of code , local variables, and the information of API invocation, separately, for semantic code search.</small>
</p>

<p><b>On the Evaluation of Commit Message Generation Models: An Experimental Study</b> 
<br><small>
<i>ICSME 2021  (<b>CCF-B</b>) <a href="https://ieeexplore.ieee.org/abstract/document/9609189">[pdf]</a> <a href="https://github.com/DeepSoftwareAnalytics/CommitMsgEmpirical">[code]</a></i>
<br />
 Wei tao, Yanlin Wang, <u>Ensheng Shi</u>, Lun Du, Shi Han, Dongmei Zhang, Wenqiang Zhang
<br /><b>TLNR</b>: We conduct the empirical study on evaluated metrics and existing datasets. We
also collect a large-scale, information-rich, and multi-language
commit message dataset MCMD and evaluate existing models
on this dataset. </small>
</p>

### 2020
<p><b>CoCoGUM: Contextual Code Summarization with
Multi-Relational GNN on UMLs</b> 
<br><small>
<i>MSR-TR 2020 <a href="https://www.microsoft.com/en-us/research/uploads/prod/2020/05/CoCoGUM-TR.pdf">[pdf]</a> </i>
<br />
Yanlin Wang, Lun Du, <u>Ensheng Shi</u>, Yuxuan Hu, Shi Han, Dongmei Zhang
<br /><b>TLNR</b>: We explore modeling two global
contexts: intra-class level context and inter-class level context for code summarization.</small>
</p>

## Educations

 <td align="left"><h3>
2019.8 ~ Present: <a href="http://en.xjtu.edu.cn/">Xi'an Jiaotong University</a> </h3>
<ul>
<li><p>MSRA-XJTU Joint PHD</p>
</li>
<li><p>The College of Artificial Intelligence</p>
</li>
</ul>
</td>

<td align="left"><h3>
2015.8 ~ 2019.7: <a href="http://en.xjtu.edu.cn/">Xi'an Jiaotong University</a> </h3>
<ul>
<li><p>Outstanding Graduate</p>
</li>
<li><p>Automatic Science and Technology</p>
</li>
</ul>
</td>

## Experiences
<ul>
<li>
Research Intern in Microsoft Research Asia<br>
Advised by Dongmei Zhang, Shi Han, & Yanlin Wang in Data, Knowledge, and Intelligence group, from Jun 2020 to Present.
</li>
<li>
Research Intern in Microsoft Research Asia<br>
Advised by Dongmei Zhang, Shi Han, Zhouyu Fu, & Mengyu Zhou in Software Analytics group, from Nov 2018 to Aug 2019.

</li>
</ul>

## Awards
<ul>
<li>
2019 Outstanding Graduate Award 
</li>
<li>
2018 Elite Class Scholarship of Institute of Automation, China Academy of Sciences
</li>
<li>
2018 Grateful Scientist Bursary
</li>
<li>
2018 National Scholarship
</li>
<li>
2017 Outstanding Volunteer Award
</li>
<li>2017 National  Encouragement Scholarship
</li>
<li>
2016 First Prize of Mathematical Modeling Contest at Provincial Level
</li>
<li>2016 National  Encouragement Scholarship
</li>
</ul>
