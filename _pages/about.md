---
permalink: /
title: "About me"
excerpt: "About me"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---
I am a joint PhD student, supervised by Prof. Hongbin Sun and Dongmei Zhang, Shi han, Yanlin Wang, between Xi'an Jiaotong University and Microsoft Research Asia. 

I am mainly focusing on code intelligence (intersection of Software Engineering and Artificial Intelligence.), which aims to leverage AI to help software developers improve the productivity of the development and reduce costs of the maintenance process. Specifically, it utilizes the **machine learning** model to mine knowledge from large scale, free source code data (Big Code) which is available on Github, etc, obtains the better **code representation** (based on code token/AST/PDG/IR etc) and applies the representation to the **downstream tasks** such as code summarization, code search, clone detection, bug detection,  code completion, program repai, etc.

My research areas currently include: **(1) Code Represention Learning; (2) Code Summarization; (3) Code Search.**



## Publication
2021

**CAST: Enhancing Code Summarization with Hierarchical Splitting and Reconstruction of Abstract Syntax Trees** 
 <font size=2>

ICSE2022 [paper]() [code]()

<u>Ensheng Shi</u>, Yanlin Wang, Lun Du, Junjie Chen, Shi Han, Hongyu Zhang, Dongmei Zhang, Hongbin Sun 

TLNR(Too Long Not to Read): Some interesting and surprising findings on the evaluated metric, code-preprocessing, and evaluated datasets. Building a shared code summarization toolbox giving actionable suggestions on the evaluation of neural code summarization. 
<details>
<summary>More</summary>
<br>

- BLEU_DC (sentence BLEU with smoothing method 4) is most correlated to human perception on the evaluation of neural code summarization model among the 6 widely used BLEU variants.
<br>

- Performing S (identifier splitting) is always significantly better than not performing it. And different code pre-processing has a large impact on performance (-18\% to +25\%)
<br>

- To more comprehensively evaluate different models, it is recommended to use multiple datasets, as rank among models can be inconsistent on different datasets.
<br>

- More findings of the evaluated metric, code pre-processing operations, evaluated datasets(the data size, splitting way, and duplication ratio )
</details>
</font>

<br>

**CAST: Enhancing Code Summarization with Hierarchical Splitting and Reconstruction of Abstract Syntax Trees** 
 <font size=2>

EMNLP2022 [paper](https://aclanthology.org/2021.emnlp-main.332.pdf) [code](https://github.com/DeepSoftwareAnalytics/CAST)

<u>Ensheng Shi</u>, Yanlin Wang, Lun Du, Hongyu Zhang, Shi Han, Dongmei Zhang, Hongbin Sun 

TLNR: We hierarchically
splits and reconstructs ASTs to obtain the better code representation for code summarization. 
</font>



## Educations

## Experience

## Award

